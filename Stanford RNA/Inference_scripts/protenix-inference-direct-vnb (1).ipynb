{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87793,"databundleVersionId":11553390,"sourceType":"competition"},{"sourceId":11411364,"sourceType":"datasetVersion","datasetId":6896780},{"sourceId":11479957,"sourceType":"datasetVersion","datasetId":7195134}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\nfrom pathlib import Path\n\n# Assuming you've mounted your dataset containing the wheels and Protenix code\nDATASET_PATH = '/kaggle/input/required-presets'\n\n# Install dependencies from wheels\nwheel_path = os.path.join(DATASET_PATH, 'wheels')\n!pip install -qqq --no-index --find-links {wheel_path} torch numpy pandas scipy rdkit protenix\n\n# Add Protenix to Python path\nprotenix_path = os.path.join(DATASET_PATH, 'Protenix')\nsys.path.append(protenix_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:01:30.120707Z","iopub.execute_input":"2025-04-20T12:01:30.120974Z","iopub.status.idle":"2025-04-20T12:04:11.437815Z","shell.execute_reply.started":"2025-04-20T12:01:30.120953Z","shell.execute_reply":"2025-04-20T12:04:11.436911Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.26.3 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.26.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.26.3 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.3.1 which is incompatible.\ntorchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd \nvalidation_sequence = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/validation_sequences.csv')\nvalidation_labels = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/validation_labels.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:04:41.533213Z","iopub.execute_input":"2025-04-20T12:04:41.533535Z","iopub.status.idle":"2025-04-20T12:04:42.202786Z","shell.execute_reply.started":"2025-04-20T12:04:41.533506Z","shell.execute_reply":"2025-04-20T12:04:42.202044Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"test_sequences = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/train_sequences.csv') \ntest_sequences.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:04:44.503145Z","iopub.execute_input":"2025-04-20T12:04:44.503426Z","iopub.status.idle":"2025-04-20T12:04:44.573284Z","shell.execute_reply.started":"2025-04-20T12:04:44.503405Z","shell.execute_reply":"2025-04-20T12:04:44.572616Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"  target_id                            sequence temporal_cutoff  \\\n0    1SCL_A       GGGUGCUCAGUACGAGAGGAACCGCACCC      1995-01-26   \n1    1RNK_A  GGCGCAGUGGGCUAGCGCCACUCAAAAGGCCCAU      1995-02-27   \n2    1RHT_A            GGGACUGACGAUCACGCAGUCUAU      1995-06-03   \n3    1HLX_A                GGGAUAACUUCGGUUGUCCC      1995-09-15   \n4    1HMH_E  GGCGACCCUGAUGAGGCCGAAAGGCCGAAACCGU      1995-12-07   \n\n                                         description  \\\n0               THE SARCIN-RICIN LOOP, A MODULAR RNA   \n1  THE STRUCTURE OF AN RNA PSEUDOKNOT THAT CAUSES...   \n2  24-MER RNA HAIRPIN COAT PROTEIN BINDING SITE F...   \n3  P1 HELIX NUCLEIC ACIDS (DNA/RNA) RIBONUCLEIC ACID   \n4  THREE-DIMENSIONAL STRUCTURE OF A HAMMERHEAD RI...   \n\n                                       all_sequences  \n0  >1SCL_1|Chain A|RNA SARCIN-RICIN LOOP|Rattus n...  \n1  >1RNK_1|Chain A|RNA PSEUDOKNOT|null\\nGGCGCAGUG...  \n2  >1RHT_1|Chain A|RNA (5'-R(P*GP*GP*GP*AP*CP*UP*...  \n3  >1HLX_1|Chain A|RNA (5'-R(*GP*GP*GP*AP*UP*AP*A...  \n4  >1HMH_1|Chains A, C, E|HAMMERHEAD RIBOZYME-RNA...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target_id</th>\n      <th>sequence</th>\n      <th>temporal_cutoff</th>\n      <th>description</th>\n      <th>all_sequences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1SCL_A</td>\n      <td>GGGUGCUCAGUACGAGAGGAACCGCACCC</td>\n      <td>1995-01-26</td>\n      <td>THE SARCIN-RICIN LOOP, A MODULAR RNA</td>\n      <td>&gt;1SCL_1|Chain A|RNA SARCIN-RICIN LOOP|Rattus n...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1RNK_A</td>\n      <td>GGCGCAGUGGGCUAGCGCCACUCAAAAGGCCCAU</td>\n      <td>1995-02-27</td>\n      <td>THE STRUCTURE OF AN RNA PSEUDOKNOT THAT CAUSES...</td>\n      <td>&gt;1RNK_1|Chain A|RNA PSEUDOKNOT|null\\nGGCGCAGUG...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1RHT_A</td>\n      <td>GGGACUGACGAUCACGCAGUCUAU</td>\n      <td>1995-06-03</td>\n      <td>24-MER RNA HAIRPIN COAT PROTEIN BINDING SITE F...</td>\n      <td>&gt;1RHT_1|Chain A|RNA (5'-R(P*GP*GP*GP*AP*CP*UP*...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1HLX_A</td>\n      <td>GGGAUAACUUCGGUUGUCCC</td>\n      <td>1995-09-15</td>\n      <td>P1 HELIX NUCLEIC ACIDS (DNA/RNA) RIBONUCLEIC ACID</td>\n      <td>&gt;1HLX_1|Chain A|RNA (5'-R(*GP*GP*GP*AP*UP*AP*A...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1HMH_E</td>\n      <td>GGCGACCCUGAUGAGGCCGAAAGGCCGAAACCGU</td>\n      <td>1995-12-07</td>\n      <td>THREE-DIMENSIONAL STRUCTURE OF A HAMMERHEAD RI...</td>\n      <td>&gt;1HMH_1|Chains A, C, E|HAMMERHEAD RIBOZYME-RNA...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/stanford-rna-3d-folding/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:04:53.400228Z","iopub.execute_input":"2025-04-20T12:04:53.400527Z","iopub.status.idle":"2025-04-20T12:04:53.411060Z","shell.execute_reply.started":"2025-04-20T12:04:53.400505Z","shell.execute_reply":"2025-04-20T12:04:53.410281Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:04:57.603145Z","iopub.execute_input":"2025-04-20T12:04:57.603419Z","iopub.status.idle":"2025-04-20T12:04:57.621520Z","shell.execute_reply.started":"2025-04-20T12:04:57.603400Z","shell.execute_reply":"2025-04-20T12:04:57.620765Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"        ID resname  resid  x_1  y_1  z_1  x_2  y_2  z_2  x_3  y_3  z_3  x_4  \\\n0  R1107_1       G      1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1  R1107_2       G      2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2  R1107_3       G      3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3  R1107_4       G      4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4  R1107_5       G      5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n   y_4  z_4  x_5  y_5  z_5  \n0  0.0  0.0  0.0  0.0  0.0  \n1  0.0  0.0  0.0  0.0  0.0  \n2  0.0  0.0  0.0  0.0  0.0  \n3  0.0  0.0  0.0  0.0  0.0  \n4  0.0  0.0  0.0  0.0  0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>resname</th>\n      <th>resid</th>\n      <th>x_1</th>\n      <th>y_1</th>\n      <th>z_1</th>\n      <th>x_2</th>\n      <th>y_2</th>\n      <th>z_2</th>\n      <th>x_3</th>\n      <th>y_3</th>\n      <th>z_3</th>\n      <th>x_4</th>\n      <th>y_4</th>\n      <th>z_4</th>\n      <th>x_5</th>\n      <th>y_5</th>\n      <th>z_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>R1107_1</td>\n      <td>G</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>R1107_2</td>\n      <td>G</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>R1107_3</td>\n      <td>G</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>R1107_4</td>\n      <td>G</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>R1107_5</td>\n      <td>G</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import urllib.request\nurllib.request.urlopen(\"https://www.google.com\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:27:23.339503Z","iopub.execute_input":"2025-04-20T12:27:23.339810Z","iopub.status.idle":"2025-04-20T12:27:23.406610Z","shell.execute_reply.started":"2025-04-20T12:27:23.339790Z","shell.execute_reply":"2025-04-20T12:27:23.405933Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<http.client.HTTPResponse at 0x7c1614efb370>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"\"\"\"\nRNA 3D Structure Prediction and Submission Generator for Kaggle Environment\n\nThis script:\n1. Reads RNA sequences from test_sequences.csv\n2. Creates input JSONs for each sequence\n3. Runs the Protenix model to predict 3D structures\n4. Extracts C1' atom coordinates from the output CIF files\n5. Creates a submission.csv file in the format required\n\"\"\"\n\nimport os\nimport json\nimport subprocess\nimport pandas as pd\nimport numpy as np\nimport glob\nfrom tqdm import tqdm\nfrom biotite.structure.io import pdbx\n\ndef create_input_json(sequence, target_id):\n    \"\"\"\n    Create the input JSON for a single RNA sequence\n    \"\"\"\n    input_json = [{\n        \"sequences\": [\n            {\n                \"rnaSequence\": {\n                    \"sequence\": sequence,\n                    \"count\": 1,\n                    \"modifications\": []\n                }\n            }\n        ],\n        \"name\": target_id,\n        \"covalent_bonds\": []\n    }]\n    return input_json\n\ndef run_inference(input_json_path, output_dir, target_id):\n    \"\"\"\n    Run inference using the Protenix model with kaggle-specific paths\n    \"\"\"\n    # Ensure output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Define command with kaggle-specific paths\n    cmd = [\n        \"python\", \"/kaggle/input/required-presets/Protenix/runner/inference.py\",\n        \"--seeds\", \"42\",\n        \"--dump_dir\", output_dir,\n        \"--input_json_path\", input_json_path,\n        \"--model.N_cycle\", \"10\",\n        \"--sample_diffusion.N_sample\", \"5\",\n        \"--sample_diffusion.N_step\", \"200\",\n        \"--load_checkpoint_path\", \"/kaggle/input/required-presets/Protenix/release_data/checkpoint/model_v0.2.0.pt\",\n        \"--use_deepspeed_evo_attention\", \"false\"\n    ]\n    \n    print(f\"Running inference for {target_id}\")\n    try:\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(f\"Error running inference for {target_id}: {result.stderr}\")\n            return False\n        return True\n    except Exception as e:\n        print(f\"Exception running inference for {target_id}: {e}\")\n        return False\n\ndef extract_c1_coordinates(cif_file_path):\n    \"\"\"\n    Extract C1' atom coordinates from a CIF file using biotite\n    \"\"\"\n    try:\n        # Read the CIF file using the correct biotite method\n        with open(cif_file_path, 'r') as f:\n            cif_data = pdbx.CIFFile.read(f)\n        \n        # Get structure from CIF data\n        atom_array = pdbx.get_structure(cif_data, model=1)\n        \n        # Clean atom names and find C1' atoms\n        atom_names_clean = np.char.strip(atom_array.atom_name.astype(str))\n        mask_c1 = atom_names_clean == \"C1'\"\n        c1_atoms = atom_array[mask_c1]\n        \n        if len(c1_atoms) == 0:\n            print(f\"Warning: No C1' atoms found in {cif_file_path}\")\n            return None\n        \n        # Sort by residue ID and return coordinates\n        sort_indices = np.argsort(c1_atoms.res_id)\n        c1_atoms_sorted = c1_atoms[sort_indices]\n        c1_coords = c1_atoms_sorted.coord\n        \n        return c1_coords\n    except Exception as e:\n        print(f\"Error extracting C1' coordinates from {cif_file_path}: {e}\")\n        return None\n\n\ndef process_sequence(sequence, target_id, temp_dir, output_dir):\n    \"\"\"\n    Process a single RNA sequence and return C1' coordinates\n    \"\"\"\n    print(f\"Processing {target_id}: {sequence}\")\n    \n    # Create input JSON\n    input_json = create_input_json(sequence, target_id)\n    \n    # Save JSON to temporary file\n    os.makedirs(temp_dir, exist_ok=True)\n    input_json_path = os.path.join(temp_dir, f\"{target_id}_input.json\")\n    with open(input_json_path, \"w\") as f:\n        json.dump(input_json, f, indent=4)\n    \n    # Run inference\n    success = run_inference(input_json_path, output_dir, target_id)\n    \n    if not success:\n        print(f\"Inference failed for {target_id}\")\n        return None\n    \n    # Find the CIF files for this target\n    target_prediction_dir = os.path.join(output_dir, target_id, \"seed_42\", \"predictions\")\n    if not os.path.exists(target_prediction_dir):\n        print(f\"Prediction directory not found for {target_id}\")\n        return None\n    \n    # Look for CIF files with the pattern {target_id}_seed_42_sample_*.cif\n    cif_files = sorted(glob.glob(os.path.join(target_prediction_dir, f\"{target_id}_seed_42_sample_*.cif\")))\n    \n    # If no CIF files found, return None\n    if not cif_files:\n        print(f\"No CIF files found for {target_id}\")\n        return None\n    \n    print(f\"Found {len(cif_files)} CIF files for {target_id}\")\n    \n    # Extract C1' coordinates from each CIF file\n    all_coords = []\n    for cif_file in cif_files:\n        coords = extract_c1_coordinates(cif_file)\n        if coords is not None:\n            all_coords.append(coords)\n    \n    if not all_coords:\n        print(f\"No valid C1' coordinates found for {target_id}\")\n        return None\n    \n    # Ensure we have 5 models (if we have fewer, duplicate the last one)\n    while len(all_coords) < 5:\n        print(f\"Only {len(all_coords)} models found for {target_id}, duplicating last model\")\n        all_coords.append(all_coords[-1])\n    \n    return all_coords[:5]  # Ensure we only have 5 models\n\ndef create_submission(test_sequences_df, c1_coords_dict, output_file):\n    \"\"\"\n    Create the submission CSV file with C1' coordinates\n    \"\"\"\n    rows = []\n    \n    # Process each sequence\n    for _, row in test_sequences_df.iterrows():\n        target_id = row['target_id']\n        sequence = row['sequence']\n        \n        if target_id not in c1_coords_dict or c1_coords_dict[target_id] is None:\n            print(f\"No prediction found for {target_id}, using zeros\")\n            # Create empty predictions (all zeros)\n            for i, residue in enumerate(sequence):\n                row_data = {\n                    'ID': f\"{target_id}_{i+1}\",\n                    'resname': residue,\n                    'resid': i+1\n                }\n                for model in range(1, 6):\n                    row_data[f'x_{model}'] = 0.0\n                    row_data[f'y_{model}'] = 0.0\n                    row_data[f'z_{model}'] = 0.0\n                rows.append(row_data)\n        else:\n            # Get the 5 models for this target\n            models = c1_coords_dict[target_id]\n            \n            # Create a row for each residue\n            for i, residue in enumerate(sequence):\n                row_data = {\n                    'ID': f\"{target_id}_{i+1}\",\n                    'resname': residue,\n                    'resid': i+1\n                }\n                \n                # Add coordinates for each model\n                for model_idx in range(5):\n                    if model_idx < len(models) and i < len(models[model_idx]):\n                        row_data[f'x_{model_idx+1}'] = models[model_idx][i][0]\n                        row_data[f'y_{model_idx+1}'] = models[model_idx][i][1]\n                        row_data[f'z_{model_idx+1}'] = models[model_idx][i][2]\n                    else:\n                        # If coordinates are not available, use zeros\n                        row_data[f'x_{model_idx+1}'] = 0.0\n                        row_data[f'y_{model_idx+1}'] = 0.0\n                        row_data[f'z_{model_idx+1}'] = 0.0\n                \n                rows.append(row_data)\n    \n    # Create DataFrame and save to CSV\n    df = pd.DataFrame(rows)\n    df.to_csv(output_file, index=False)\n    print(f\"Created submission file: {output_file}\")\n\ndef main():\n    \"\"\"\n    Main function\n    \"\"\"\n    # Set up required symlinks for CCD cache as in kaggle_inference.py\n    os.makedirs(\"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache\", exist_ok=True)\n    \n    source_ccd_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif\"\n    target_ccd_file = \"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache/components.v20240608.cif\"\n    \n    source_rdkit_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif.rdkit_mol.pkl\"\n    target_rdkit_file = \"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache/components.v20240608.cif.rdkit_mol.pkl\"\n    \n    # Create the symlinks if the source files exist\n    if os.path.exists(source_ccd_file) and not os.path.exists(target_ccd_file):\n        try:\n            os.symlink(source_ccd_file, target_ccd_file)\n            print(f\"Created symlink for CCD file\")\n        except Exception as e:\n            print(f\"Error creating symlink for CCD file: {e}\")\n    \n    if os.path.exists(source_rdkit_file) and not os.path.exists(target_rdkit_file):\n        try:\n            os.symlink(source_rdkit_file, target_rdkit_file)\n            print(f\"Created symlink for RDKIT file\")\n        except Exception as e:\n            print(f\"Error creating symlink for RDKIT file: {e}\")\n    \n    # Create directories\n    temp_dir = \"./input\"  # Same as in kaggle_inference.py\n    output_dir = \"./output\"  # Same as in kaggle_inference.py\n    os.makedirs(temp_dir, exist_ok=True)\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Read test sequences\n    test_sequences_df = pd.read_csv(\"/kaggle/input/stanford-rna-3d-folding/test_sequences.csv\")\n    print(f\"Loaded {len(test_sequences_df)} test sequences\")\n    \n    # Process each sequence\n    c1_coords_dict = {}\n    for _, row in tqdm(test_sequences_df.iterrows(), total=len(test_sequences_df)):\n        target_id = row['target_id']\n        sequence = row['sequence']\n        \n        # Check if we already have predictions for this target\n        target_prediction_dir = os.path.join(output_dir, target_id, \"seed_42\", \"predictions\")\n        if os.path.exists(target_prediction_dir):\n            print(f\"Found existing prediction for {target_id}, loading coordinates\")\n            # Extract coordinates from existing predictions\n            cif_files = sorted(glob.glob(os.path.join(target_prediction_dir, f\"{target_id}_seed_42_sample_*.cif\")))\n            \n            all_coords = []\n            for cif_file in cif_files:\n                coords = extract_c1_coordinates(cif_file)\n                if coords is not None:\n                    all_coords.append(coords)\n            \n            if all_coords:\n                # Ensure we have 5 models\n                while len(all_coords) < 5:\n                    all_coords.append(all_coords[-1])\n                c1_coords_dict[target_id] = all_coords[:5]\n                continue\n        \n        # Process the sequence if no existing prediction was found or was invalid\n        c1_coords = process_sequence(sequence, target_id, temp_dir, output_dir)\n        c1_coords_dict[target_id] = c1_coords\n    \n    # Create submission file\n    create_submission(test_sequences_df, c1_coords_dict, \"submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T12:32:12.926499Z","iopub.execute_input":"2025-04-20T12:32:12.926821Z","iopub.status.idle":"2025-04-20T12:32:32.359590Z","shell.execute_reply.started":"2025-04-20T12:32:12.926799Z","shell.execute_reply":"2025-04-20T12:32:32.358303Z"}},"outputs":[{"name":"stdout","text":"Loaded 12 test sequences\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/12 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Processing R1107: GGGGGCCACAGCAGAAGCGUUCACGUCGCAGCCCCUGUCAGCCAUUGCACUCCGGCUGCGAAUUCUGCU\nRunning inference for R1107\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 1/12 [00:06<01:12,  6.57s/it]","output_type":"stream"},{"name":"stdout","text":"Error running inference for R1107: Traceback (most recent call last):\n  File \"/kaggle/input/required-presets/Protenix/runner/inference.py\", line 305, in <module>\n    run()\n  File \"/kaggle/input/required-presets/Protenix/runner/inference.py\", line 300, in run\n    download_infercence_cache(configs, model_version=\"v0.2.0\")\n  File \"/kaggle/input/required-presets/Protenix/runner/inference.py\", line 184, in download_infercence_cache\n    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n  File \"/usr/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 30] Read-only file system: '/kaggle/input/required-presets/Protenix/release_data'\n\nInference failed for R1107\nProcessing R1108: GGGGGCCACAGCAGAAGCGUUCACGUCGCGGCCCCUGUCAGCCAUUGCACUCCGGCUGCGAAUUCUGCU\nRunning inference for R1108\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 2/12 [00:13<01:05,  6.52s/it]","output_type":"stream"},{"name":"stdout","text":"Error running inference for R1108: Traceback (most recent call last):\n  File \"/kaggle/input/required-presets/Protenix/runner/inference.py\", line 305, in <module>\n    run()\n  File \"/kaggle/input/required-presets/Protenix/runner/inference.py\", line 300, in run\n    download_infercence_cache(configs, model_version=\"v0.2.0\")\n  File \"/kaggle/input/required-presets/Protenix/runner/inference.py\", line 184, in download_infercence_cache\n    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n  File \"/usr/lib/python3.10/os.py\", line 215, in makedirs\n    makedirs(head, exist_ok=exist_ok)\n  File \"/usr/lib/python3.10/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 30] Read-only file system: '/kaggle/input/required-presets/Protenix/release_data'\n\nInference failed for R1108\nProcessing R1116: CGCCCGGAUAGCUCAGUCGGUAGAGCAGCGGCUAAAACAGCUCUGGGGUUGUACCCACCCCAGAGGCCCACGUGGCGGCUAGUACUCCGGUAUUGCGGUACCCUUGUACGCCUGUUUUAGCCGCGGGUCCAGGGUUCAAGUCCCUGUUCGGGCGCCA\nRunning inference for R1116\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 2/12 [00:19<01:36,  9.69s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-23dcf1720dad>\u001b[0m in \u001b[0;36m<cell line: 286>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-23dcf1720dad>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;31m# Process the sequence if no existing prediction was found or was invalid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mc1_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0mc1_coords_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc1_coords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-23dcf1720dad>\u001b[0m in \u001b[0;36mprocess_sequence\u001b[0;34m(sequence, target_id, temp_dir, output_dir)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_json_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-23dcf1720dad>\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(input_json_path, output_dir, target_id)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running inference for {target_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error running inference for {target_id}: {result.stderr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2019\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:25:33.391803Z","iopub.execute_input":"2025-04-04T14:25:33.392158Z","iopub.status.idle":"2025-04-04T14:25:33.424164Z","shell.execute_reply.started":"2025-04-04T14:25:33.392131Z","shell.execute_reply":"2025-04-04T14:25:33.423395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# # Check if the components.cif file exists\n# ccd_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif\"\n# rdkit_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif.rdkit_mol.pkl\"\n\n# print(f\"CCD file exists: {os.path.exists(ccd_file)}\")\n# print(f\"RDKIT file exists: {os.path.exists(rdkit_file)}\")\n\n# # If they don't exist, let's look for them\n# if not (os.path.exists(ccd_file) and os.path.exists(rdkit_file)):\n#     print(\"Searching for CCD files in /kaggle/input/required-presets/Protenix/release_data/...\")\n#     for root, dirs, files in os.walk(\"/kaggle/input/required-presets/Protenix/release_data/\"):\n#         for file in files:\n#             if \"components\" in file and (\"cif\" in file or \"rdkit\" in file):\n#                 print(f\"Found: {os.path.join(root, file)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:39:03.501642Z","iopub.status.idle":"2025-04-04T13:39:03.501895Z","shell.execute_reply":"2025-04-04T13:39:03.501788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import subprocess\n\n# # Create the directory structure needed\n# os.makedirs(\"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache\", exist_ok=True)\n\n# # Create symlinks to the actual files\n# source_ccd_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif\"\n# target_ccd_file = \"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache/components.v20240608.cif\"\n\n# source_rdkit_file = \"/kaggle/input/required-presets/Protenix/release_data/ccd_cache/components.v20240608.cif.rdkit_mol.pkl\"\n# target_rdkit_file = \"/usr/local/lib/python3.10/dist-packages/release_data/ccd_cache/components.v20240608.cif.rdkit_mol.pkl\"\n\n# # Check if the source files exist\n# print(f\"Source CCD file exists: {os.path.exists(source_ccd_file)}\")\n# print(f\"Source RDKIT file exists: {os.path.exists(source_rdkit_file)}\")\n\n# # Create the symlinks if the source files exist\n# if os.path.exists(source_ccd_file):\n#     try:\n#         os.symlink(source_ccd_file, target_ccd_file)\n#         print(f\"Created symlink for CCD file\")\n#     except FileExistsError:\n#         print(f\"Symlink for CCD file already exists\")\n#     except Exception as e:\n#         print(f\"Error creating symlink for CCD file: {e}\")\n# else:\n#     print(f\"Cannot create symlink, source CCD file doesn't exist\")\n\n# if os.path.exists(source_rdkit_file):\n#     try:\n#         os.symlink(source_rdkit_file, target_rdkit_file)\n#         print(f\"Created symlink for RDKIT file\")\n#     except FileExistsError:\n#         print(f\"Symlink for RDKIT file already exists\")\n#     except Exception as e:\n#         print(f\"Error creating symlink for RDKIT file: {e}\")\n# else:\n#     print(f\"Cannot create symlink, source RDKIT file doesn't exist\")\n\n# # Create RNA input JSON\n# input_json = [{\n#     \"sequences\": [\n#         {\n#             \"rnaSequence\": {\n#                 \"sequence\": \"GGGUGCUCAGUACGAGAGGAACCGCACCC\",\n#                 \"count\": 1,\n#                 \"modifications\": []\n#             }\n#         }\n#     ],\n#     \"name\": \"rna_prediction\",\n#     \"covalent_bonds\": []\n# }]\n\n# # Save input JSON\n# os.makedirs(\"./input\", exist_ok=True)\n# with open(\"./input/rna_input.json\", \"w\") as f:\n#     json.dump(input_json, f, indent=4)\n\n# # Run inference using subprocess\n# cmd = [\n#     \"python\", \"/kaggle/input/required-presets/Protenix/runner/inference.py\",\n#     \"--seeds\", \"42\",\n#     \"--dump_dir\", \"./output\",\n#     \"--input_json_path\", \"./input/rna_input.json\",\n#     \"--model.N_cycle\", \"10\",\n#     \"--sample_diffusion.N_sample\", \"5\",\n#     \"--sample_diffusion.N_step\", \"200\",\n#     \"--load_checkpoint_path\", \"/kaggle/input/required-presets/Protenix/release_data/checkpoint/model_v0.2.0.pt\",\n#     \"--use_deepspeed_evo_attention\", \"false\"\n# ]\n\n# # Run the command\n# result = subprocess.run(cmd, capture_output=True, text=True)\n# print(\"STDOUT:\", result.stdout)\n# print(\"STDERR:\", result.stderr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:39:03.50271Z","iopub.status.idle":"2025-04-04T13:39:03.50311Z","shell.execute_reply":"2025-04-04T13:39:03.50295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check for error files\n# if os.path.exists(\"./output/ERR\"):\n#     print(\"Error directory exists!\")\n#     print(\"Contents:\")\n#     for item in os.listdir(\"./output/ERR\"):\n#         print(f\" - {item}\")\n    \n#     # If there are error files, show their contents\n#     error_files = os.listdir(\"./output/ERR\")\n#     if error_files:\n#         with open(os.path.join(\"./output/ERR\", error_files[0]), \"r\") as f:\n#             print(f\"Contents of {error_files[0]}:\")\n#             print(f.read())\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:43:02.500381Z","iopub.execute_input":"2025-04-04T10:43:02.500601Z","iopub.status.idle":"2025-04-04T10:43:02.506377Z","shell.execute_reply.started":"2025-04-04T10:43:02.500582Z","shell.execute_reply":"2025-04-04T10:43:02.505427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import pandas as pd\n# import biotite.structure.io.pdbx as pdbx\n# import matplotlib.pyplot as plt\n# from mpl_toolkits.mplot3d import Axes3D\n# import numpy as np\n\n# # List all the prediction files\n# output_dir = \"/kaggle/working/output/rna_prediction/seed_42/predictions/\"\n# cif_files = [f for f in os.listdir(output_dir) if f.endswith(\".cif\")]\n# print(f\"Found {len(cif_files)} prediction files:\")\n# for file in cif_files:\n#     print(f\" - {file}\")\n\n# # Read and analyze the first prediction file\n# if cif_files:\n#     # Read CIF file\n#     cif_file = os.path.join(output_dir, cif_files[0])\n#     with open(cif_file, 'r') as f:\n#         cif_data = pdbx.CIFFile.read(f)\n\n#     atom_array = pdbx.get_structure(cif_data, model=1)\n    \n#     print(f\"\\nStructure information for {cif_files[0]}:\")\n#     print(f\"Number of atoms: {len(atom_array)}\")\n#     print(f\"Residue count: {len(np.unique(atom_array.res_id))}\")\n\n#     # Clean and extract C1' atoms\n#     atom_names_clean = np.char.strip(atom_array.atom_name.astype(str))\n#     mask_c1 = atom_names_clean == \"C1'\"\n#     c1_atoms = atom_array[mask_c1]\n#     c1_coords = c1_atoms.coord\n\n#     print(f\"\\nFound {len(c1_atoms)} C1' atoms\")\n\n#     # Create DataFrame\n#     df = pd.DataFrame({\n#         \"res_name\": c1_atoms.res_name,\n#         \"res_id\": c1_atoms.res_id,\n#         \"chain_id\": c1_atoms.chain_id,\n#         \"x\": c1_coords[:, 0],\n#         \"y\": c1_coords[:, 1],\n#         \"z\": c1_coords[:, 2]\n#     })\n\n#     print(\"\\nFirst few C1' atoms:\")\n#     print(df.head())\n\n#     # Save to CSV and JSON\n#     df.to_csv(\"c1_prime_coordinates.csv\", index=False)\n#     df.to_json(\"c1_prime_coordinates.json\", orient=\"records\", indent=2)\n#     print(\"Saved C1' coordinates to CSV and JSON.\")\n\n#     # Plot C1' atoms with backbone\n#     fig = plt.figure(figsize=(10, 8))\n#     ax = fig.add_subplot(111, projection='3d')\n\n#     chain_ids = np.unique(c1_atoms.chain_id)\n#     colors = plt.cm.rainbow(np.linspace(0, 1, len(chain_ids)))\n\n#     for i, chain_id in enumerate(chain_ids):\n#         chain_mask = c1_atoms.chain_id == chain_id\n#         chain_df = df[df[\"chain_id\"] == chain_id]\n#         chain_df_sorted = chain_df.sort_values(\"res_id\")\n\n#         ax.scatter(\n#             chain_df_sorted[\"x\"],\n#             chain_df_sorted[\"y\"],\n#             chain_df_sorted[\"z\"],\n#             c=[colors[i]],\n#             label=f\"Chain {chain_id}\",\n#             alpha=0.9,\n#             s=30\n#         )\n\n#         ax.plot(\n#             chain_df_sorted[\"x\"],\n#             chain_df_sorted[\"y\"],\n#             chain_df_sorted[\"z\"],\n#             color=colors[i],\n#             alpha=0.6,\n#             linewidth=2\n#         )\n\n#     ax.set_xlabel(\"X (Å)\")\n#     ax.set_ylabel(\"Y (Å)\")\n#     ax.set_zlabel(\"Z (Å)\")\n#     ax.set_title(f\"C1' atom backbone - {cif_files[0]}\")\n#     ax.legend()\n#     plt.tight_layout()\n#     plt.show()\n\n#     # --- NEW: Plot all atoms in another image ---\n#     print(\"\\nGenerating full atom visualization...\")\n\n#     all_coords = atom_array.coord\n#     all_chain_ids = np.unique(atom_array.chain_id)\n#     colors_all = plt.cm.rainbow(np.linspace(0, 1, len(all_chain_ids)))\n\n#     fig_all = plt.figure(figsize=(10, 8))\n#     ax_all = fig_all.add_subplot(111, projection='3d')\n\n#     for i, chain_id in enumerate(all_chain_ids):\n#         chain_mask = atom_array.chain_id == chain_id\n#         ax_all.scatter(\n#             all_coords[chain_mask, 0],\n#             all_coords[chain_mask, 1],\n#             all_coords[chain_mask, 2],\n#             c=[colors_all[i]],\n#             label=f\"Chain {chain_id}\",\n#             alpha=0.6,\n#             s=5  # smaller for all atoms\n#         )\n\n#     ax_all.set_xlabel(\"X (Å)\")\n#     ax_all.set_ylabel(\"Y (Å)\")\n#     ax_all.set_zlabel(\"Z (Å)\")\n#     ax_all.set_title(f\"All atoms - {cif_files[0]}\")\n#     ax_all.legend()\n#     plt.tight_layout()\n#     plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:53:14.639027Z","iopub.execute_input":"2025-04-04T10:53:14.639415Z","iopub.status.idle":"2025-04-04T10:53:15.084099Z","shell.execute_reply.started":"2025-04-04T10:53:14.639385Z","shell.execute_reply":"2025-04-04T10:53:15.083226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install nglview","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:56:30.789313Z","iopub.execute_input":"2025-04-04T10:56:30.789587Z","iopub.status.idle":"2025-04-04T10:57:06.942843Z","shell.execute_reply.started":"2025-04-04T10:56:30.789566Z","shell.execute_reply":"2025-04-04T10:57:06.941693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n# import os\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# # Path to the summary confidence JSON file\n# confidence_file = \"/kaggle/working/output/rna_prediction/seed_42/predictions/rna_prediction_seed_42_summary_confidence_sample_0.json\"\n\n# # Read and display the file contents\n# with open(confidence_file, 'r') as f:\n#     confidence_data = json.load(f)\n\n# # Show the keys in the JSON file\n# print(\"Keys in the confidence file:\")\n# for key in confidence_data.keys():\n#     print(f\" - {key}\")\n\n# # Function to display and visualize specific metrics\n# def analyze_metric(data, metric_name):\n#     if metric_name in data:\n#         print(f\"\\n{metric_name} data:\")\n        \n#         # Handle different data types\n#         if isinstance(data[metric_name], (int, float)):\n#             print(f\"{metric_name}: {data[metric_name]}\")\n        \n#         elif isinstance(data[metric_name], list):\n#             print(f\"{metric_name} (first 5 values): {data[metric_name][:5]}\")\n            \n#             # Plot if it's a list of numbers\n#             if data[metric_name] and isinstance(data[metric_name][0], (int, float)):\n#                 plt.figure(figsize=(10, 6))\n#                 plt.plot(data[metric_name])\n#                 plt.title(f\"{metric_name} across residues\")\n#                 plt.xlabel(\"Residue index\")\n#                 plt.ylabel(metric_name)\n#                 plt.grid(True, alpha=0.3)\n#                 plt.savefig(f\"{metric_name}_plot.png\")\n#                 plt.close()\n#                 print(f\"Saved visualization to '{metric_name}_plot.png'\")\n        \n#         elif isinstance(data[metric_name], dict):\n#             # For dictionaries, show keys and sample values\n#             print(f\"{metric_name} contains {len(data[metric_name])} keys:\")\n#             for k in list(data[metric_name].keys())[:5]:\n#                 print(f\"  - {k}: {data[metric_name][k]}\")\n#     else:\n#         print(f\"\\n{metric_name} not found in the data\")\n\n# # Analyze common confidence metrics\n# analyze_metric(confidence_data, \"plddt\")  # Per-residue confidence scores\n# analyze_metric(confidence_data, \"ranking_score\")  # Overall ranking score of the model\n# analyze_metric(confidence_data, \"gpde\")  # Global predicted distance error\n\n# # If there's PAE (Predicted Aligned Error) matrix, visualize it\n# if \"pae\" in confidence_data and isinstance(confidence_data[\"pae\"], list):\n#     pae_data = np.array(confidence_data[\"pae\"])\n    \n#     if len(pae_data.shape) == 2:\n#         plt.figure(figsize=(10, 8))\n#         im = plt.imshow(pae_data, cmap='viridis_r')\n#         plt.colorbar(im, label=\"Predicted Aligned Error (Å)\")\n#         plt.title(\"PAE Matrix\")\n#         plt.xlabel(\"Residue\")\n#         plt.ylabel(\"Residue\")\n#         plt.savefig(\"pae_matrix.png\")\n#         plt.close()\n#         print(\"\\nSaved PAE matrix visualization to 'pae_matrix.png'\")\n\n# # Display the structure's overall quality assessment\n# print(\"\\nOverall structure quality assessment:\")\n# quality_metrics = [\"ranking_score\", \"gpde\", \"plddt_avg\", \"pae_avg\"]\n# for metric in quality_metrics:\n#     if metric in confidence_data:\n#         print(f\" - {metric}: {confidence_data[metric]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:43:03.559591Z","iopub.execute_input":"2025-04-04T10:43:03.55982Z","iopub.status.idle":"2025-04-04T10:43:03.574372Z","shell.execute_reply.started":"2025-04-04T10:43:03.5598Z","shell.execute_reply":"2025-04-04T10:43:03.573528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n# import pprint\n\n# # Path to the summary confidence JSON file\n# confidence_file = \"/kaggle/working/output/rna_prediction/seed_42/predictions/rna_prediction_seed_42_summary_confidence_sample_0.json\"\n\n# # Read and display the complete file contents\n# with open(confidence_file, 'r') as f:\n#     confidence_data = json.load(f)\n\n# # Use pretty print to display formatted JSON\n# print(\"Complete JSON content:\")\n# pp = pprint.PrettyPrinter(indent=2)\n# pp.pprint(confidence_data)\n\n# # Or alternatively, print it with json.dumps for more control over formatting\n# print(\"\\nAlternative JSON formatting:\")\n# print(json.dumps(confidence_data, indent=2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T10:43:03.575142Z","iopub.execute_input":"2025-04-04T10:43:03.575459Z","iopub.status.idle":"2025-04-04T10:43:03.598221Z","shell.execute_reply.started":"2025-04-04T10:43:03.575428Z","shell.execute_reply":"2025-04-04T10:43:03.597511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}